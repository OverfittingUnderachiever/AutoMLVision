{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing freeze-thaw bayesian optimization with two-level Gaussian processes\n",
    "# A global GP models the asymptotic mean of the learning curves of each HP-config\n",
    "# Local GPs model the learning curves of each HP-config\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, Kernel, Sum, WhiteKernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ft_testfunction import global_xD, local_xD\n",
    "import scipy as sc\n",
    "\n",
    "# Hyper-Hyperparameters of Freeze-Thaw Bayesian Optimization\n",
    "ALPHA,BETA = 1,0.5\n",
    "NOISE = 0.1\n",
    "B_OLD=2 # 10\n",
    "B_NEW=4 # 3\n",
    "MAX_EPOCHS=100\n",
    "EPOCH_STEP=1\n",
    "N_SAMPLES_MC=10 # number of samples for Monte Carlo integration\n",
    "N_FANT=1 # 5 # number of observations we fantasize for each point in the basket\n",
    "N_INIT_CONFIGS=10 # number of random initializations for the optimization\n",
    "N_INIT_EPOCHS=5 # number of epochs trained for initial configs\n",
    "INFERRED_MEAN = 0.8 # inferred mean of the global GP\n",
    "MATER_NU = 2.5 # Matern kernel parameter\n",
    "EI_N_SAMPLES = 1000 #10000 # number of samples for EI optimization\n",
    "\n",
    "# Meta-Parameters of the task\n",
    "OBS_MIN = 0 # minimal loss value\n",
    "OBS_MAX = 1 # maximal loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquation 3: Expected Improvement-formula\\nEquation 4: Entropy Search-formula\\nEquation 5&6: Exponential Decay-kernel for Gaussian Process\\nEquation 12,13,17 & 18: Posterior distribution of global Gaussian Process\\nEquation 14 & 19: Posterior predictive distribution of global Gaussian Process\\nEquation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\\nEquation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equation 3: Expected Improvement-formula\n",
    "Equation 4: Entropy Search-formula\n",
    "Equation 5&6: Exponential Decay-kernel for Gaussian Process\n",
    "Equation 12,13,17 & 18: Posterior distribution of global Gaussian Process\n",
    "Equation 14 & 19: Posterior predictive distribution of global Gaussian Process\n",
    "Equation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\n",
    "Equation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of exponential-decay kernel for local GPs\n",
    "class ExponentialDecayNoiseKernel(Kernel):\n",
    "    def __init__(self, alpha=1.0,beta=0.5,noise=0.1):\n",
    "        self.beta=beta\n",
    "        self.alpha=alpha\n",
    "        self.noise = noise\n",
    "    def __call__(self, X, Y=None,eval_gradient=False):\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        # if eval_gradient:\n",
    "        #     return (self.beta**self.alpha)/((X.flatten()[:,None]+Y.flatten()[None,:])+self.beta)**self.alpha,np.identity(X.shape[0])\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        return ((self.beta**self.alpha)/((X[None,:]+Y[:,None])+self.beta)**self.alpha + self.noise*np.where(X[None,:]-Y[:,None] == 0, 1, 0)).T\n",
    "    def diag(self, X):\n",
    "        return np.diag(self(X))\n",
    "    def is_stationary(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.11_1.34': (array([1., 2., 3., 4., 5.]), array([0.95, 0.91, 0.86, 0.81, 0.77])), '4.23_2.82': (array([1., 2., 3., 4., 5.]), array([0.95, 0.87, 0.83, 0.78, 0.74])), '4.99_1.05': (array([1., 2., 3., 4., 5.]), array([0.98, 0.94, 0.93, 0.9 , 0.87])), '4.5_1.74': (array([1., 2., 3., 4., 5.]), array([0.95, 0.91, 0.87, 0.81, 0.8 ])), '2.96_3.56': (array([1., 2., 3., 4., 5.]), array([0.93, 0.84, 0.81, 0.76, 0.7 ])), '3.04_4.69': (array([1., 2., 3., 4., 5.]), array([0.92, 0.85, 0.81, 0.74, 0.69])), '1.79_4.4': (array([1., 2., 3., 4., 5.]), array([0.93, 0.88, 0.83, 0.79, 0.75])), '4.95_1.07': (array([1., 2., 3., 4., 5.]), array([0.98, 0.95, 0.94, 0.92, 0.89])), '2.01_3.02': (array([1., 2., 3., 4., 5.]), array([0.95, 0.88, 0.81, 0.77, 0.74])), '4.74_4.72': (array([1., 2., 3., 4., 5.]), array([0.96, 0.92, 0.89, 0.86, 0.84]))}\n",
      "[[1.11 1.34]\n",
      " [4.23 2.82]\n",
      " [4.99 1.05]\n",
      " [4.5  1.74]\n",
      " [2.96 3.56]\n",
      " [3.04 4.69]\n",
      " [1.79 4.4 ]\n",
      " [4.95 1.07]\n",
      " [2.01 3.02]\n",
      " [4.74 4.72]]\n"
     ]
    }
   ],
   "source": [
    "# Start by observing N_INIT_CONFIGS random configurations for N_INIT_EPOCHS epoch each\n",
    "bounds={'HP1':(1.,5.),'HP2':(1.,5.)}\n",
    "observed_configs_dicts={}\n",
    "observed_configs_list=[]\n",
    "for i in range(N_INIT_CONFIGS):\n",
    "    new_config=np.empty(0)\n",
    "    for key in bounds.keys():\n",
    "        new_config=np.append(new_config,np.round(np.random.uniform(bounds[key][0],bounds[key][1]),2))\n",
    "    # Observe the new configuration for N_INIT_EPOCHS epochs\n",
    "    f_space = np.linspace(1,N_INIT_EPOCHS,N_INIT_EPOCHS)\n",
    "    experimental_data=local_xD(np.array(new_config),f_space,noise=0.01)\n",
    "    observed_configs_dicts['_'.join([str(config) for config in new_config])]=(f_space,experimental_data)\n",
    "\n",
    "\n",
    "    observed_configs_list.append(new_config)\n",
    "observed_configs_list=np.array(observed_configs_list)\n",
    "\n",
    "\n",
    "\n",
    "print(observed_configs_dicts)\n",
    "print(observed_configs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from botorch.models import SingleTaskGP\n",
    "# from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "# from gpytorch.means import ConstantMean\n",
    "# import torch\n",
    "#MaternKernel(nu=2.5)\n",
    "# print(torch.from_numpy(observed_configs_list))\n",
    "\n",
    "\n",
    "# Define the kernel for the global GP\n",
    "kernel_global = Matern(nu=MATER_NU)\n",
    "kernel_local = ExponentialDecayNoiseKernel(alpha=ALPHA,beta=BETA,noise=NOISE)\n",
    "\n",
    "def compute_k_x(x,new_x,kernel):\n",
    "    k_x = kernel_global(x)\n",
    "    k_x_star = kernel_global(x,new_x)\n",
    "    k_star_star = kernel_global(new_x)\n",
    "    return k_x,k_x_star,k_star_star\n",
    "\n",
    "def compute_k_t_n(x,new_x,kernel):\n",
    "    k_t=kernel(x)\n",
    "    k_t_star=kernel(x,new_x)\n",
    "    k_star_star=kernel(new_x)\n",
    "    return k_t,k_t_star,k_star_star\n",
    "\n",
    "def compute_k_t(x,x_dict):\n",
    "    k_ts=[]\n",
    "    for x_n in x:\n",
    "        k_ts.append(kernel_local(x_dict['_'.join([str(c) for c in x_n])][0]))\n",
    "    return sc.linalg.block_diag(*k_ts)\n",
    "\n",
    "def compute_o(config_list,config_dict):\n",
    "    o=[]\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        o=sc.linalg.block_diag(*([o,np.ones((len(observations),1))]))\n",
    "    return o[1:]\n",
    "\n",
    "def compute_lambda(o,k_t_inv):\n",
    "    return o.T@k_t_inv@o\n",
    "\n",
    "def compute_gamma(o,k_t_inv,y,m):\n",
    "    return o.T@k_t_inv@(y-o@m)\n",
    "\n",
    "def constant_mean(x):\n",
    "    return INFERRED_MEAN*np.ones(x.shape[0])\n",
    "\n",
    "def compute_y_vector(config_list,config_dict):\n",
    "    y_vec=np.empty(0)\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        y_vec=np.append(y_vec,observations)\n",
    "    return y_vec\n",
    "\n",
    "def compute_c(k_x_inv,lambd):\n",
    "    return np.linalg.inv(k_x_inv+lambd)\n",
    "\n",
    "def compute_omega(k_t_n_star,k_t_n_inv):\n",
    "    obs_steps=k_t_n_inv.shape[0]\n",
    "    predict_steps=k_t_n_star.shape[1]\n",
    "    return np.ones(predict_steps)-k_t_n_star.T@k_t_n_inv@np.ones(obs_steps)\n",
    "\n",
    "def compute_mu(m,c,gamma):\n",
    "    return (m+c@gamma)\n",
    "\n",
    "# Equation 14/19:\n",
    "def compute_mu_x_star(m,k_x_star,k_x_inv,mu,means_vec):\n",
    "    return m+k_x_star.T@k_x_inv@(mu-means_vec)\n",
    "def compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x,lambd_inv):\n",
    "    return k_x_star_star-k_x_star.T@np.linalg.inv(k_x+lambd_inv)@k_x_star\n",
    "\n",
    "# Equation 16/21:\n",
    "def compute_sigma_n_star_new(k_t_star_star,sigma_star_star):\n",
    "    return k_t_star_star-sigma_star_star\n",
    "\n",
    "# Equation 15/20:\n",
    "def compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,y_n,omega_n,mu_n):\n",
    "    return k_t_n_star.T@k_t_n_inv@y_n+(omega_n*mu_n)\n",
    "def compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,c_nn):\n",
    "    return k_t_n_star_star-k_t_n_star.T@k_t_n_inv@k_t_n_star+omega_n@(c_nn*omega_n.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.11 1.34]\n",
      " [4.23 2.82]\n",
      " [4.99 1.05]\n",
      " [4.5  1.74]\n",
      " [2.96 3.56]\n",
      " [3.04 4.69]\n",
      " [1.79 4.4 ]\n",
      " [4.95 1.07]\n",
      " [2.01 3.02]\n",
      " [4.74 4.72]]\n",
      "[[1.11 2.  ]\n",
      " [3.   2.  ]\n",
      " [1.   2.  ]]\n",
      "[1.11 1.34]\n",
      "μx*:\n",
      "[0.8057105  0.78969852 0.80628098]\n",
      "Σx**:\n",
      "[[ 0.45448727  0.01268031  0.45542951]\n",
      " [ 0.01268031  0.79849291 -0.00256883]\n",
      " [ 0.45542951 -0.00256883  0.47529131]]\n",
      "μn* (existing):\n",
      "[0.81317427]\n",
      "Σn* (existing):\n",
      "[[0.17646856]]\n"
     ]
    }
   ],
   "source": [
    "new_x = np.array([[observed_configs_list[0,0],2],[3,2],[1,2]]) # Placeholer\n",
    "print(observed_configs_list)\n",
    "print(new_x)\n",
    "k_x,k_x_star,k_x_star_star = compute_k_x(observed_configs_list,new_x,kernel_global)\n",
    "k_x_inv = np.linalg.inv(k_x)\n",
    "# print(f\"Kx:\\n{k_x}\\n Kx*:\\n{k_x_star}\\n Kx**:\\n{k_x_star_star}\")\n",
    "\n",
    "new_x_n=np.array([100]) # Placeholer\n",
    "curve=observed_configs_dicts[\"_\".join([str(x) for x in observed_configs_list[0]])]\n",
    "print(observed_configs_list[0])\n",
    "# print(curve)\n",
    "# print(new_x_n)\n",
    "k_t_n,k_t_n_star,k_t_n_star_star=compute_k_t_n(curve[0],new_x_n,kernel_local)\n",
    "k_t_n_inv=np.linalg.inv(k_t_n)\n",
    "# print(f\"Ktn:\\n{k_t_n}\\n Ktn*:\\n{k_t_n_star}\\n Ktn**:\\n{k_t_n_star_star}\")\n",
    "\n",
    "k_t=compute_k_t(observed_configs_list,observed_configs_dicts) # Placeholer\n",
    "k_t_inv=np.linalg.inv(k_t)\n",
    "# print(f\"Kt:\\n {k_t}\")\n",
    "\n",
    "o = compute_o(observed_configs_list,observed_configs_dicts)\n",
    "# print(f\"O:\\n{o}\")\n",
    "\n",
    "lambd = compute_lambda(o,k_t_inv)\n",
    "lambd_inv = np.linalg.inv(lambd)\n",
    "# print(f\"Lambda:\\n{lambd}\")\n",
    "\n",
    "means_vec = constant_mean(observed_configs_list)\n",
    "# print(f\"Means:\\n{means_vec}\")\n",
    "\n",
    "y_vec = compute_y_vector(observed_configs_list,observed_configs_dicts)\n",
    "# print(f\"Y:\\n{y_vec}\")\n",
    "\n",
    "gamma = compute_gamma(o,k_t_inv,y_vec,means_vec)\n",
    "# print(f\"Gamma:\\n{gamma}\")\n",
    "\n",
    "c = compute_c(k_x_inv,lambd)\n",
    "# print(f\"C:\\n{c}\")\n",
    "\n",
    "mu_global = compute_mu(means_vec,c,gamma)\n",
    "# print(f\"Mu:\\n{mu_global}\")\n",
    "\n",
    "omega_n = compute_omega(k_t_n_star,k_t_n_inv)\n",
    "# print(f\"Omega n:\\n{omega_n}\")\n",
    "\n",
    "mu = compute_mu_x_star(constant_mean(new_x),k_x_star,k_x_inv,mu_global,means_vec)\n",
    "print(f\"μx*:\\n{mu}\")\n",
    "\n",
    "var = compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x,lambd_inv)\n",
    "print(f\"Σx**:\\n{var}\")\n",
    "\n",
    "mu_n_star_ex = compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,curve[1],omega_n,mu_global[0])\n",
    "print(f\"μn* (existing):\\n{mu_n_star_ex}\")\n",
    "\n",
    "sigma_n_star_ex = compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,c[0,0])\n",
    "print(f\"Σn* (existing):\\n{sigma_n_star_ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_EI_at_x(mu,var,best_mu):\n",
    "    z=(best_mu-mu)/np.sqrt(var)\n",
    "    return np.sqrt(var)*(z*sc.stats.norm.cdf(z))+sc.stats.norm.pdf(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New basket:\n",
      "[[3.   4.25]\n",
      " [2.98 4.34]\n",
      " [2.86 4.43]\n",
      " [3.05 4.28]]\n",
      "Old basket:\n",
      "[[3.04 4.69]\n",
      " [2.96 3.56]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Our Bayesian optimization strategy proceeds by maintaining a basket of B = Bold + Bnew candidate\n",
    "models. Bold represents some number of models that have already been trained to some degree, while Bnew\n",
    "represents some number of brand new models. In practice, we set Bold = 10 and Bnew = 3. The entire\n",
    "basket is chosen using models with the maximum EI at the asymptote, which is computed using Equations\n",
    "19 and 3. Each round, after a new observation has been collected, the basket is re-built using possibly\n",
    "different models. This step is essentially standard Bayesian optimization using EI.\n",
    "'''\n",
    "\n",
    "# Fill the basket with configs\n",
    "basket_new=np.empty((0,len(bounds.keys())))\n",
    "basket_old=np.empty((0,len(bounds.keys())))\n",
    "\n",
    "# Get the best yet observed configuration\n",
    "best_observation = np.min(np.concatenate([observed_configs_dicts['_'.join([str(c) for c in config])][1] for config in observed_configs_list]))\n",
    "# print(f\"Best observation: {best_observation}\")\n",
    "\n",
    "# Calculate EI for many configs to find the best ones\n",
    "while not (len(basket_new)==B_NEW) and not (len(basket_old)==B_OLD):\n",
    "\n",
    "    # Sample N_EI_SAMPLES new configurations\n",
    "    ei_configs = []\n",
    "    for i in range(EI_N_SAMPLES):\n",
    "        while True:\n",
    "            new_config=np.empty(0)\n",
    "            for key in bounds.keys():\n",
    "                new_config=np.append(new_config,np.round(np.random.uniform(bounds[key][0],bounds[key][1]),2))\n",
    "            if not new_config in observed_configs_list:\n",
    "                break\n",
    "        ei_configs.append(new_config)\n",
    "    if len(basket_old)<B_OLD:\n",
    "        ei_configs = np.concatenate([ei_configs,observed_configs_list])\n",
    "    ei_configs=np.array(ei_configs)\n",
    "    # print(ei_configs)\n",
    "\n",
    "    # Calculate the mean and variance at the asymptote for each config using equation 19\n",
    "    k_x,k_x_star,k_x_star_star = compute_k_x(observed_configs_list,ei_configs,kernel_global)\n",
    "    k_x_inv = np.linalg.inv(k_x)\n",
    "    k_t=compute_k_t(observed_configs_list,observed_configs_dicts)\n",
    "    k_t_inv=np.linalg.inv(k_t)\n",
    "    means_vec = constant_mean(observed_configs_list)\n",
    "    o=compute_o(observed_configs_list,observed_configs_dicts)\n",
    "    lambd = compute_lambda(o,k_t_inv)\n",
    "    c=compute_c(k_x_inv,lambd)\n",
    "    y_vec = compute_y_vector(observed_configs_list,observed_configs_dicts)\n",
    "    gamma = compute_gamma(o,k_t_inv,y_vec,means_vec)\n",
    "    mu_global = compute_mu(means_vec,c,gamma)\n",
    "    mu = compute_mu_x_star(constant_mean(ei_configs),k_x_star,k_x_inv,mu_global,means_vec)\n",
    "    # print(f\"μx*:\\n{mu}\")\n",
    "    cov = compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x,lambd_inv)\n",
    "    var=np.diag(cov)\n",
    "    # print(f\"Σx**:\\n{var}\")\n",
    "\n",
    "    # Calculate the EI scores for each config using equation 3\n",
    "    ei_scores=compute_EI_at_x(mu,var,best_observation)\n",
    "    sort_indices = np.argsort(ei_scores)[::-1]\n",
    "    ei_configs_ranked = ei_configs[sort_indices]\n",
    "    # print(ei_configs_ranked)\n",
    "\n",
    "\n",
    "    # Greedily choose the best HP-config using Equation 19 & 3 until B_OLD existing configs and B_NEW new configs are found\n",
    "\n",
    "    for sampled_EI_config in ei_configs_ranked:\n",
    "        # Sample another config from EI and try to add it to the basket\n",
    "        # If it is already in the basket, or the basket is full, skip it\n",
    "        if not np.any(np.all(np.isin(observed_configs_list,sampled_EI_config),axis=1)) and (basket_new.shape[0]==0 or B_NEW>basket_new.shape[0] and not np.any(np.all(np.isin(basket_new,sampled_EI_config),axis=1))):\n",
    "            # print(f\"Adding new config to basket_new: {sampled_EI_config}\")\n",
    "            basket_new=np.vstack([basket_new,sampled_EI_config])\n",
    "        elif np.any(np.all(np.isin(observed_configs_list,sampled_EI_config),axis=1)) and (basket_old.shape[0]==0 or B_OLD>basket_old.shape[0] and not np.any(np.all(np.isin(basket_old,sampled_EI_config),axis=1))):\n",
    "            # print(f\"Adding new config to basket_old: {sampled_EI_config}\")\n",
    "            basket_old=np.vstack([basket_old,sampled_EI_config])\n",
    "\n",
    "\n",
    "print(f\"New basket:\\n{basket_new}\")\n",
    "print(f\"Old basket:\\n{basket_old}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute p_min over the basket using Monte Carlo simulation and Equation 19\n",
    "\n",
    "# p_min is the posterior predictive distribution over the minimum of the global GP\n",
    "# We will use it's entropy to observe configs with results that make the p_min distribution spikier\n",
    "\n",
    "# Monte Carlo simulation\n",
    "# Using the global GP, we create a discrete grid of the N_SAMPLES_MC highest EI configs\n",
    "# From the grid, compute P_min and the entropy of P_min, H(P_min)\n",
    "\n",
    "h_p_min=1\n",
    "a=np.zeros(basket_new.shape[0]+basket_old.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen new config: [3.   4.25]\n",
      "Chosen new config: [2.98 4.34]\n",
      "Chosen new config: [2.86 4.43]\n",
      "Chosen new config: [3.05 4.28]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,2) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m         a[k\u001b[38;5;241m+\u001b[39mB_NEW]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(h_p_min_y\u001b[38;5;241m-\u001b[39mh_p_min)\u001b[38;5;241m/\u001b[39mN_FANT\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Select the config with the highest information gain\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m best_config\u001b[38;5;241m=\u001b[39m(\u001b[43mbasket_new\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbasket_old\u001b[49m)[np\u001b[38;5;241m.\u001b[39margmax(a)]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_config)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,2) (2,2) "
     ]
    }
   ],
   "source": [
    "# For each config in the basket, N_FANT times fantasize an observation and recompute the information gain from it, collecting it in a\n",
    "for k,config in enumerate(basket_new):\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 21\n",
    "        # TODO fantasize an observation\n",
    "        print(f\"Chosen new config: {config}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        # Compute p_min_y, the new minimum distribution, including the fantasized observation y, again using Monte Carlo and Equation 19\n",
    "        # TODO compute p_min_y\n",
    "\n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        # TODO compute H(P_min_y)\n",
    "        h_p_min_y=0.5\n",
    "        # \n",
    "        a[k]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "for k,config in enumerate(basket_old):\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 20\n",
    "        # TODO fantasize an observation\n",
    "\n",
    "        # Compute p_min_y, the new minimum distribution, including the fantasized observation y, again using Monte Carlo and Equation 19\n",
    "        # TODO compute p_min_y\n",
    "\n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        # TODO compute H(P_min_y)\n",
    "        h_p_min_y=0\n",
    "\n",
    "        a[k+B_NEW]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "\n",
    "# Select the config with the highest information gain\n",
    "best_config=(basket_new+basket_old)[np.argmax(a)]\n",
    "print(best_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_11_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
