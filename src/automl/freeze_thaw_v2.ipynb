{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing freeze-thaw bayesian optimization with two-level Gaussian processes\n",
    "# A global GP models the asymptotic mean of the learning curves of each HP-config\n",
    "# Local GPs model the learning curves of each HP-config\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, Kernel, Sum, WhiteKernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ft_testfunction import global_xD, local_xD\n",
    "import scipy as sc\n",
    "from freeze_thaw import FreezeThaw, init_configs\n",
    "\n",
    "# Hyper-Hyperparameters of Freeze-Thaw Bayesian Optimization\n",
    "ALPHA,BETA = 1,0.5\n",
    "NOISE = 0.1\n",
    "B_OLD=10 # 10\n",
    "B_NEW=3 # 3\n",
    "N_SAMPLES_MC=1000 # number of samples for Monte Carlo integration\n",
    "N_FANT=5 # 5 # number of observations we fantasize for each point in the basket\n",
    "N_INIT_CONFIGS=10 # number of random initializations for the optimization\n",
    "N_INIT_EPOCHS=5 # number of epochs trained for initial configs\n",
    "INFERRED_MEAN = 0.8 # inferred mean of the global GP\n",
    "MATER_NU = 2.5 # Matern kernel parameter\n",
    "EI_N_SAMPLES = 1000 #10000 # number of samples for EI optimization\n",
    "PRED_EPOCH = 1 # how many epochs to predict for\n",
    "\n",
    "# Meta-Parameters of the task\n",
    "OBS_MIN = 0 # minimal loss value\n",
    "OBS_MAX = 1 # maximal loss value\n",
    "bounds={'HP1':(1.,5.),'HP2':(1.,5.)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of exponential-decay kernel for local GPs\n",
    "class ExponentialDecayNoiseKernel(Kernel):\n",
    "    def __init__(self, alpha=1.0,beta=0.5,noise=0.1):\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.noise = noise\n",
    "    def __call__(self, X, Y=None,eval_gradient=False):\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        return ((self.beta**self.alpha)/((X[None,:]+Y[:,None])+self.beta)**self.alpha + self.noise*np.where(X[None,:]-Y[:,None] == 0, 1, 0)).T\n",
    "    def diag(self, X):\n",
    "        return np.diag(self(X))\n",
    "    def is_stationary(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_global = Matern(nu=MATER_NU)\n",
    "kernel_local = ExponentialDecayNoiseKernel(alpha=ALPHA,beta=BETA,noise=NOISE)\n",
    "\n",
    "# Start by observing N_INIT_CONFIGS random configurations for N_INIT_EPOCHS epoch each\n",
    "observed_configs_dicts={}\n",
    "observed_configs_list=[]\n",
    "for i in range(N_INIT_CONFIGS):\n",
    "    new_config=np.empty(0)\n",
    "    for key in bounds.keys():\n",
    "        new_config=np.append(new_config,np.round(np.random.uniform(bounds[key][0],bounds[key][1]),2))\n",
    "    # Observe the new configuration for N_INIT_EPOCHS epochs\n",
    "    f_space = np.linspace(1,N_INIT_EPOCHS,N_INIT_EPOCHS)\n",
    "    experimental_data=local_xD(new_config,f_space,noise=0.01)\n",
    "    observed_configs_dicts['_'.join([str(config) for config in new_config])]=(f_space,experimental_data)\n",
    "    observed_configs_list.append(new_config)\n",
    "\n",
    "observed_configs_list=np.array(observed_configs_list)\n",
    "# print(observed_configs_dicts)\n",
    "# print(observed_configs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ks(x,new_x,kernel):\n",
    "    k_x = kernel(x)\n",
    "    k_x_star = kernel(x,new_x)\n",
    "    k_star_star = kernel(new_x)\n",
    "    return k_x,k_x_star,k_star_star\n",
    "\n",
    "def compute_k_star_star(new_x,kernel):\n",
    "    return kernel(new_x)\n",
    "\n",
    "def compute_k_t(x,x_dict):\n",
    "    k_ts=[]\n",
    "    for x_n in x:\n",
    "        k_ts.append(kernel_local(x_dict['_'.join([str(c) for c in x_n])][0]))\n",
    "    return sc.linalg.block_diag(*k_ts)\n",
    "\n",
    "def compute_o(config_list,config_dict):\n",
    "    o=[]\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        o=sc.linalg.block_diag(*([o,np.ones((len(observations),1))]))\n",
    "    return o[1:]\n",
    "\n",
    "def compute_lambda(o,k_t_inv):\n",
    "    return o.T@k_t_inv@o\n",
    "\n",
    "def compute_gamma(o,k_t_inv,y,m):\n",
    "    return o.T@k_t_inv@(y-o@m)\n",
    "\n",
    "def constant_mean(x):\n",
    "    return INFERRED_MEAN*np.ones(x.shape[0])\n",
    "\n",
    "def compute_y_vector(config_list,config_dict):\n",
    "    y_vec=np.empty(0)\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        y_vec=np.append(y_vec,observations)\n",
    "    return y_vec\n",
    "\n",
    "def compute_c(k_x_inv,lambd):\n",
    "    return np.linalg.inv(k_x_inv+lambd)\n",
    "\n",
    "def compute_omega(k_t_n_star,k_t_n_inv):\n",
    "    obs_steps=k_t_n_inv.shape[0]\n",
    "    predict_steps=k_t_n_star.shape[1]\n",
    "    return np.ones(predict_steps)-k_t_n_star.T@k_t_n_inv@np.ones(obs_steps)\n",
    "\n",
    "def compute_mu(m,c,gamma):\n",
    "    return (m+c@gamma)\n",
    "\n",
    "# Equation 14/19:\n",
    "def compute_mu_x_star(m,k_x_star,k_x_inv,mu,means_vec):\n",
    "    return m+k_x_star.T@k_x_inv@(mu-means_vec)\n",
    "def compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x,lambd_inv):\n",
    "    return k_x_star_star-k_x_star.T@np.linalg.inv(k_x+lambd_inv)@k_x_star\n",
    "\n",
    "# Equation 16/21:\n",
    "def compute_mu_n_star_new(mu_n,x_new):\n",
    "    return mu_n*np.ones(x_new.shape[0])\n",
    "def compute_sigma_n_star_new(k_t_star_star,sigma_star_star):\n",
    "    return k_t_star_star+np.identity(k_t_star_star.shape[0])*sigma_star_star\n",
    "\n",
    "# Equation 15/20:\n",
    "def compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,y_n,omega_n,mu_n):\n",
    "    return k_t_n_star.T@k_t_n_inv@y_n+(omega_n*mu_n)\n",
    "def compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,c_nn):\n",
    "    return k_t_n_star_star-k_t_n_star.T@k_t_n_inv@k_t_n_star+omega_n@(c_nn*omega_n.T)\n",
    "\n",
    "def compute_entropy(mu_vector,var_vector,n_samples):\n",
    "    var_mat=np.diag(var_vector)\n",
    "    mc_bins=np.zeros(mu_vector.shape[0])\n",
    "    for i in range(n_samples):\n",
    "        global_gp_samples = np.random.multivariate_normal(mu_vector,var_mat)\n",
    "        mc_bins[np.argmin(global_gp_samples)]+=1/n_samples\n",
    "    return sc.stats.entropy(mc_bins)\n",
    "\n",
    "def compute_EI_at_x(mu,var,best_mu):\n",
    "    z=(best_mu-mu)/np.sqrt(var)\n",
    "    return np.sqrt(var)*(z*sc.stats.norm.cdf(z))+sc.stats.norm.pdf(z)\n",
    "\n",
    "def compute_mu_var_cov_c(observed_configs_list,observed_configs_dicts,ei_configs,kernel_global):\n",
    "    # Calculate the mean and variance at the asymptote for each config using equation 19\n",
    "    k_x_n,k_x_star,k_x_star_star = compute_ks(observed_configs_list,ei_configs,kernel_global)\n",
    "    k_x_inv = np.linalg.inv(k_x_n)\n",
    "    k_t=compute_k_t(observed_configs_list,observed_configs_dicts)\n",
    "    k_t_inv=np.linalg.inv(k_t)\n",
    "    means_vec = constant_mean(observed_configs_list)\n",
    "    o=compute_o(observed_configs_list,observed_configs_dicts)\n",
    "    lambd = compute_lambda(o,k_t_inv)\n",
    "    lambd_inv = np.linalg.inv(lambd)\n",
    "    c=compute_c(k_x_inv,lambd)\n",
    "    y_vec = compute_y_vector(observed_configs_list,observed_configs_dicts)\n",
    "    gamma = compute_gamma(o,k_t_inv,y_vec,means_vec)\n",
    "    mu_global = compute_mu(means_vec,c,gamma)\n",
    "    mu = compute_mu_x_star(constant_mean(ei_configs),k_x_star,k_x_inv,mu_global,means_vec)\n",
    "    cov = compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x_n,lambd_inv)\n",
    "    var=np.diag(cov)\n",
    "    return mu,var,cov,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μx*:\n",
      "[0.85990629 0.83717645 0.78749387 ... 0.81764492 0.75830992 0.82092059]\n",
      "Σx**:\n",
      "[0.48398387 0.54562367 0.64246455 ... 0.08243816 0.07910974 0.08282435]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fill the basket with configs\n",
    "basket_new=np.empty((0,len(bounds.keys())))\n",
    "basket_old=np.empty((0,len(bounds.keys())))\n",
    "basket_new_mu_var=[]\n",
    "basket_old_mu_var=[]\n",
    "basket_old_c=[]\n",
    "\n",
    "# Get the best yet observed configuration\n",
    "best_observation = np.min(np.concatenate([observed_configs_dicts['_'.join([str(c) for c in config])][1] for config in observed_configs_list]))\n",
    "# print(f\"Best observation: {best_observation}\")\n",
    "\n",
    "# Calculate EI for many configs to find the best ones\n",
    "# Sample N_EI_SAMPLES new configurations\n",
    "ei_configs = []\n",
    "for i in range(EI_N_SAMPLES):\n",
    "    while True:\n",
    "        new_config=np.empty(0)\n",
    "        for key in bounds.keys():\n",
    "            new_config=np.append(new_config,np.round(np.random.uniform(bounds[key][0],bounds[key][1]),2))\n",
    "        if not new_config in observed_configs_list:\n",
    "            break\n",
    "    ei_configs.append(new_config)\n",
    "if len(basket_old)<B_OLD:\n",
    "    ei_configs = np.concatenate([ei_configs,observed_configs_list])\n",
    "ei_configs=np.array(ei_configs)\n",
    "# print(ei_configs)\n",
    "\n",
    "mu,var,_,c=compute_mu_var_cov_c(observed_configs_list,observed_configs_dicts,ei_configs,kernel_global)\n",
    "print(f\"μx*:\\n{mu}\")\n",
    "print(f\"Σx**:\\n{var}\")\n",
    "\n",
    "# Calculate the EI scores for each config using equation 3\n",
    "ei_scores=compute_EI_at_x(mu,var,best_observation)\n",
    "sort_indices = np.argsort(ei_scores)[::-1]\n",
    "ei_configs_ranked = ei_configs[sort_indices]\n",
    "# print(ei_configs_ranked)\n",
    "\n",
    "# Greedily choose the best HP-config using Equation 19 & 3 until B_OLD existing configs and B_NEW new configs are found\n",
    "for n_sample,sampled_EI_config in enumerate(ei_configs_ranked):\n",
    "    # Sample another config from EI and try to add it to the basket\n",
    "    # If it is already in the basket, or the basket is full, skip it\n",
    "    if not np.any(np.all(np.isin(observed_configs_list,sampled_EI_config),axis=1)) and (basket_new.shape[0]==0 or B_NEW>basket_new.shape[0] and not np.any(np.all(np.isin(basket_new,sampled_EI_config),axis=1))):\n",
    "        # print(f\"Adding new config to basket_new: {sampled_EI_config}\")\n",
    "        basket_new=np.vstack([basket_new,sampled_EI_config])\n",
    "        basket_new_mu_var.append([mu[n_sample],var[n_sample]])\n",
    "    elif np.any(np.all(np.isin(observed_configs_list,sampled_EI_config),axis=1)) and (basket_old.shape[0]==0 or B_OLD>basket_old.shape[0] and not np.any(np.all(np.isin(basket_old,sampled_EI_config),axis=1))):\n",
    "        # print(f\"Adding new config to basket_old: {sampled_EI_config}\")\n",
    "        basket_old=np.vstack([basket_old,sampled_EI_config])\n",
    "        basket_old_c.append(c[np.where((observed_configs_list==sampled_EI_config).all(axis=1)),np.where((observed_configs_list==sampled_EI_config).all(axis=1))])\n",
    "        basket_old_mu_var.append([mu[n_sample],var[n_sample]])\n",
    "\n",
    "basket_new_mu_var=np.array(basket_new_mu_var)\n",
    "basket_old_mu_var=np.array(basket_old_mu_var)\n",
    "basket_old_c=np.array(basket_old_c)\n",
    "baskets_combined=np.vstack([basket_new,basket_old])\n",
    "\n",
    "\n",
    "# print(f\"New basket:\\n{basket_new}\")\n",
    "# print(f\"Old basket:\\n{basket_old}\")\n",
    "# print(f\"New basket mu var:\\n{basket_new_mu_var}\")\n",
    "# print(f\"Old basket mu var:\\n{basket_old_mu_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of P_min: 2.4622716963228064\n"
     ]
    }
   ],
   "source": [
    "# Compute the entropy of the basket via Monte Carlo sampling\n",
    "baskets_mu_var=np.concatenate([basket_new_mu_var,basket_old_mu_var])\n",
    "h_p_min=compute_entropy(baskets_mu_var[:,0],baskets_mu_var[:,1],N_SAMPLES_MC)\n",
    "print(f\"Entropy of P_min: {h_p_min}\")\n",
    "a=np.zeros(basket_new.shape[0]+basket_old.shape[0])\n",
    "pred_epochs=np.linspace(1,PRED_EPOCH,PRED_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen new config: [2.74 3.42]\n",
      "Chosen new config: [2.75 3.57]\n",
      "Chosen new config: [2.75 3.61]\n"
     ]
    }
   ],
   "source": [
    "# For each config in the basket, N_FANT times fantasize an observation and recompute the information gain from it, collecting it in a\n",
    "for k_config,chosen_config in enumerate(basket_new):\n",
    "    \n",
    "    # Fantasize an observation using Equation 21\n",
    "    print(f\"Chosen new config: {chosen_config}\")\n",
    "    mu_n_star_new = compute_mu_n_star_new(basket_new_mu_var[k_config][0],pred_epochs)\n",
    "    # print(f\"μn* (new):\\n{mu_n_star_new}\")\n",
    "    sigma_n_star_new = compute_sigma_n_star_new(compute_k_star_star(pred_epochs,kernel_local),basket_new_mu_var[k_config][1])\n",
    "    # print(f\"Σn* (new):\\n{sigma_n_star_new}\")\n",
    "\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using the mu and sigma of the new config\n",
    "        fantasized_observation = np.random.multivariate_normal(mu_n_star_new,sigma_n_star_new)\n",
    "\n",
    "        # Compute the global mus and sigmas now including the fantasized observation\n",
    "        observations_incl_list=np.vstack([observed_configs_list,chosen_config])\n",
    "        observations_incl_dicts=observed_configs_dicts.copy()\n",
    "        observations_incl_dicts['_'.join([str(c) for c in chosen_config])]=(pred_epochs,fantasized_observation)\n",
    "\n",
    "\n",
    "        mu_y,var_y,_,_=compute_mu_var_cov_c(observations_incl_list,observations_incl_dicts,baskets_combined,kernel_global)\n",
    "        # print(f\"μx*:\\n{mu_y}\")\n",
    "        # print(f\"Σx**:\\n{var_y}\")\n",
    "\n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        h_p_min_y=compute_entropy(mu_y,var_y,N_SAMPLES_MC)\n",
    "        # print(f\"Entropy of P_min: {h_p_min_y}\")\n",
    "        a[k_config]+=(h_p_min_y-h_p_min)/N_FANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen existing config: [3.26 1.06]\n",
      "Chosen existing config: [2.54 3.92]\n",
      "Chosen existing config: [3.07 2.1 ]\n",
      "Chosen existing config: [2.37 3.09]\n",
      "Chosen existing config: [2.16 3.65]\n",
      "Chosen existing config: [1.62 4.08]\n",
      "Chosen existing config: [1.7  3.73]\n",
      "Chosen existing config: [4.42 4.13]\n",
      "Chosen existing config: [1.05 1.1 ]\n",
      "Chosen existing config: [5.   2.51]\n",
      "Best config: [2.74 3.42] (new)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k_config,chosen_config in enumerate(basket_old):\n",
    "    print(f\"Chosen existing config: {chosen_config}\")\n",
    "    pred_epochs_n=pred_epochs+observed_configs_dicts['_'.join([str(c) for c in chosen_config])][0][-1]\n",
    "    curve_n=observed_configs_dicts['_'.join([str(c) for c in chosen_config])]\n",
    "\n",
    "    k_t_n,k_t_n_star,k_t_n_star_star=compute_ks(curve_n[0],pred_epochs_n,kernel_local)\n",
    "    k_t_n_inv=np.linalg.inv(k_t_n)\n",
    "    omega_n = compute_omega(k_t_n_star,k_t_n_inv)\n",
    "    mu_n_star_ex = compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,curve_n[1],omega_n,basket_old_mu_var[k_config][0])\n",
    "    # print(f\"μn* (existing):\\n{mu_n_star_ex}\")\n",
    "    sigma_n_star_ex = compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,basket_old_c[k_config])\n",
    "    # print(f\"Σn* (existing):\\n{sigma_n_star_ex}\")\n",
    "\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 20\n",
    "        # TODO fantasize an observation\n",
    "        fantasized_observation = np.random.multivariate_normal(mu_n_star_ex,sigma_n_star_ex)\n",
    "        # print(f\"Fantasized observation: {fantasized_observation}\")\n",
    "\n",
    "        # Compute the global mus and sigmas now including the fantasized observation\n",
    "        observations_incl_list=observed_configs_list\n",
    "        observations_incl_dicts=observed_configs_dicts.copy()\n",
    "        old_config_entry=observations_incl_dicts['_'.join([str(c) for c in chosen_config])]\n",
    "        observations_incl_dicts['_'.join([str(c) for c in chosen_config])]=(np.append(old_config_entry[0],pred_epochs_n),np.append(old_config_entry[1],fantasized_observation))\n",
    "        # print(observations_incl_dicts['_'.join([str(c) for c in chosen_config])])\n",
    "\n",
    "        mu_y,var_y,_,_=compute_mu_var_cov_c(observations_incl_list,observations_incl_dicts,baskets_combined,kernel_global)\n",
    "        # print(f\"μx*:\\n{mu}\")\n",
    "        # print(f\"Σx**:\\n{var}\")\n",
    "        \n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        h_p_min_y=compute_entropy(mu_y,var_y,N_SAMPLES_MC)\n",
    "        # print(f\"Entropy of P_min: {h_p_min_y}\")\n",
    "\n",
    "        a[k_config+B_NEW]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "\n",
    "# Select the config with the highest information gain\n",
    "best_config=(np.concatenate([basket_new,basket_old]))[np.argmax(a)]\n",
    "print(f\"Best config: {best_config} ({'new' if np.argmax(a)<B_NEW else 'old'})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_11_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
