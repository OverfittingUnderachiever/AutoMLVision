{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing freeze-thaw bayesian optimization with two-level Gaussian processes\n",
    "# A global GP models the asymptotic mean of the learning curves of each HP-config\n",
    "# Local GPs model the learning curves of each HP-config\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, Kernel, Sum, WhiteKernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ft_testfunction import global_xD, local_xD\n",
    "import scipy as sc\n",
    "\n",
    "# Hyper-Hyperparameters of Freeze-Thaw Bayesian Optimization\n",
    "ALPHA,BETA = 1,0.5\n",
    "NOISE = 0.1\n",
    "B_OLD=10 # 10\n",
    "B_NEW=3 # 3\n",
    "MAX_EPOCHS=100\n",
    "EPOCH_STEP=1\n",
    "N_SAMPLES_MC=1000 # number of samples for Monte Carlo integration\n",
    "N_FANT=5 # 5 # number of observations we fantasize for each point in the basket\n",
    "N_INIT_CONFIGS=10 # number of random initializations for the optimization\n",
    "N_INIT_EPOCHS=5 # number of epochs trained for initial configs\n",
    "INFERRED_MEAN = 0.8 # inferred mean of the global GP\n",
    "MATER_NU = 2.5 # Matern kernel parameter\n",
    "EI_N_SAMPLES = 1000 #10000 # number of samples for EI optimization\n",
    "PRED_EPOCH = 1 # how many epochs to predict for\n",
    "\n",
    "# Meta-Parameters of the task\n",
    "OBS_MIN = 0 # minimal loss value\n",
    "OBS_MAX = 1 # maximal loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquation 3: Expected Improvement-formula\\nEquation 4: Entropy Search-formula\\nEquation 5&6: Exponential Decay-kernel for Gaussian Process\\nEquation 12,13,17 & 18: Posterior distribution of global Gaussian Process\\nEquation 14 & 19: Posterior predictive distribution of global Gaussian Process\\nEquation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\\nEquation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\\n'"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equation 3: Expected Improvement-formula\n",
    "Equation 4: Entropy Search-formula\n",
    "Equation 5&6: Exponential Decay-kernel for Gaussian Process\n",
    "Equation 12,13,17 & 18: Posterior distribution of global Gaussian Process\n",
    "Equation 14 & 19: Posterior predictive distribution of global Gaussian Process\n",
    "Equation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\n",
    "Equation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of exponential-decay kernel for local GPs\n",
    "class ExponentialDecayNoiseKernel(Kernel):\n",
    "    def __init__(self, alpha=1.0,beta=0.5,noise=0.1):\n",
    "        self.beta=beta\n",
    "        self.alpha=alpha\n",
    "        self.noise = noise\n",
    "    def __call__(self, X, Y=None,eval_gradient=False):\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        # if eval_gradient:\n",
    "        #     return (self.beta**self.alpha)/((X.flatten()[:,None]+Y.flatten()[None,:])+self.beta)**self.alpha,np.identity(X.shape[0])\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        return ((self.beta**self.alpha)/((X[None,:]+Y[:,None])+self.beta)**self.alpha + self.noise*np.where(X[None,:]-Y[:,None] == 0, 1, 0)).T\n",
    "    def diag(self, X):\n",
    "        return np.diag(self(X))\n",
    "    def is_stationary(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3.51_4.47': (array([1., 2., 3., 4., 5.]), array([0.93, 0.87, 0.81, 0.75, 0.71])), '4.43_3.91': (array([1., 2., 3., 4., 5.]), array([0.95, 0.89, 0.87, 0.82, 0.77])), '1.24_3.12': (array([1., 2., 3., 4., 5.]), array([0.93, 0.9 , 0.86, 0.81, 0.77])), '3.89_4.91': (array([1., 2., 3., 4., 5.]), array([0.92, 0.87, 0.82, 0.77, 0.72])), '3.99_1.65': (array([1., 2., 3., 4., 5.]), array([0.93, 0.87, 0.83, 0.78, 0.73])), '1.84_2.32': (array([1., 2., 3., 4., 5.]), array([0.94, 0.87, 0.83, 0.79, 0.75])), '2.7_2.27': (array([1., 2., 3., 4., 5.]), array([0.95, 0.85, 0.81, 0.76, 0.68])), '2.2_3.34': (array([1., 2., 3., 4., 5.]), array([0.93, 0.86, 0.81, 0.77, 0.72])), '4.6_2.24': (array([1., 2., 3., 4., 5.]), array([0.96, 0.92, 0.87, 0.84, 0.82])), '3.02_1.94': (array([1., 2., 3., 4., 5.]), array([0.91, 0.86, 0.81, 0.76, 0.7 ]))}\n",
      "[[3.51 4.47]\n",
      " [4.43 3.91]\n",
      " [1.24 3.12]\n",
      " [3.89 4.91]\n",
      " [3.99 1.65]\n",
      " [1.84 2.32]\n",
      " [2.7  2.27]\n",
      " [2.2  3.34]\n",
      " [4.6  2.24]\n",
      " [3.02 1.94]]\n"
     ]
    }
   ],
   "source": [
    "# Start by observing N_INIT_CONFIGS random configurations for N_INIT_EPOCHS epoch each\n",
    "bounds={'HP1':(1.,5.),'HP2':(1.,5.)}\n",
    "observed_configs_dicts={}\n",
    "observed_configs_list=[]\n",
    "for i in range(N_INIT_CONFIGS):\n",
    "    new_config=np.empty(0)\n",
    "    for key in bounds.keys():\n",
    "        new_config=np.append(new_config,np.round(np.random.uniform(bounds[key][0],bounds[key][1]),2))\n",
    "    # Observe the new configuration for N_INIT_EPOCHS epochs\n",
    "    f_space = np.linspace(1,N_INIT_EPOCHS,N_INIT_EPOCHS)\n",
    "    experimental_data=local_xD(np.array(new_config),f_space,noise=0.01)\n",
    "    observed_configs_dicts['_'.join([str(config) for config in new_config])]=(f_space,experimental_data)\n",
    "\n",
    "\n",
    "    observed_configs_list.append(new_config)\n",
    "observed_configs_list=np.array(observed_configs_list)\n",
    "\n",
    "\n",
    "\n",
    "print(observed_configs_dicts)\n",
    "print(observed_configs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from botorch.models import SingleTaskGP\n",
    "# from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "# from gpytorch.means import ConstantMean\n",
    "# import torch\n",
    "#MaternKernel(nu=2.5)\n",
    "# print(torch.from_numpy(observed_configs_list))\n",
    "\n",
    "\n",
    "# Define the kernel for the global GP\n",
    "kernel_global = Matern(nu=MATER_NU)\n",
    "kernel_local = ExponentialDecayNoiseKernel(alpha=ALPHA,beta=BETA,noise=NOISE)\n",
    "\n",
    "def compute_k_x(x,new_x,kernel):\n",
    "    k_x = kernel(x)\n",
    "    k_x_star = kernel(x,new_x)\n",
    "    k_star_star = kernel(new_x)\n",
    "    return k_x,k_x_star,k_star_star\n",
    "\n",
    "def compute_k_t_n(x,new_x,kernel):\n",
    "    k_t=kernel(x)\n",
    "    k_t_star=kernel(x,new_x)\n",
    "    k_star_star=kernel(new_x)\n",
    "    return k_t,k_t_star,k_star_star\n",
    "\n",
    "def compute_k_x_star_star(new_x,kernel):\n",
    "    return kernel(new_x)\n",
    "\n",
    "def compute_k_t(x,x_dict):\n",
    "    k_ts=[]\n",
    "    for x_n in x:\n",
    "        k_ts.append(kernel_local(x_dict['_'.join([str(c) for c in x_n])][0]))\n",
    "    return sc.linalg.block_diag(*k_ts)\n",
    "\n",
    "def compute_o(config_list,config_dict):\n",
    "    o=[]\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        o=sc.linalg.block_diag(*([o,np.ones((len(observations),1))]))\n",
    "    return o[1:]\n",
    "\n",
    "def compute_lambda(o,k_t_inv):\n",
    "    return o.T@k_t_inv@o\n",
    "\n",
    "def compute_gamma(o,k_t_inv,y,m):\n",
    "    return o.T@k_t_inv@(y-o@m)\n",
    "\n",
    "def constant_mean(x):\n",
    "    return INFERRED_MEAN*np.ones(x.shape[0])\n",
    "\n",
    "def compute_y_vector(config_list,config_dict):\n",
    "    y_vec=np.empty(0)\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        y_vec=np.append(y_vec,observations)\n",
    "    return y_vec\n",
    "\n",
    "def compute_c(k_x_inv,lambd):\n",
    "    return np.linalg.inv(k_x_inv+lambd)\n",
    "\n",
    "def compute_omega(k_t_n_star,k_t_n_inv):\n",
    "    obs_steps=k_t_n_inv.shape[0]\n",
    "    predict_steps=k_t_n_star.shape[1]\n",
    "    return np.ones(predict_steps)-k_t_n_star.T@k_t_n_inv@np.ones(obs_steps)\n",
    "\n",
    "def compute_mu(m,c,gamma):\n",
    "    return (m+c@gamma)\n",
    "\n",
    "# Equation 14/19:\n",
    "def compute_mu_x_star(m,k_x_star,k_x_inv,mu,means_vec):\n",
    "    return m+k_x_star.T@k_x_inv@(mu-means_vec)\n",
    "def compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x,lambd_inv):\n",
    "    return k_x_star_star-k_x_star.T@np.linalg.inv(k_x+lambd_inv)@k_x_star\n",
    "\n",
    "# Equation 16/21:\n",
    "def compute_mu_n_star_new(mu_n,x_new):\n",
    "    return mu_n*np.ones(x_new.shape[0])\n",
    "def compute_sigma_n_star_new(k_t_star_star,sigma_star_star):\n",
    "    return k_t_star_star+np.identity(k_t_star_star.shape[0])*sigma_star_star\n",
    "\n",
    "# Equation 15/20:\n",
    "def compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,y_n,omega_n,mu_n):\n",
    "    return k_t_n_star.T@k_t_n_inv@y_n+(omega_n*mu_n)\n",
    "def compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,c_nn):\n",
    "    return k_t_n_star_star-k_t_n_star.T@k_t_n_inv@k_t_n_star+omega_n@(c_nn*omega_n.T)\n",
    "\n",
    "def compute_entropy(mu_vector,var_vector,n_samples):\n",
    "    var_mat=np.diag(var_vector)\n",
    "    mc_bins=np.zeros(mu_vector.shape[0])\n",
    "    for i in range(n_samples):\n",
    "        global_gp_samples = np.random.multivariate_normal(mu_vector,var_mat)\n",
    "        mc_bins[np.argmin(global_gp_samples)]+=1/n_samples\n",
    "    return sc.stats.entropy(mc_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.51 4.47]\n",
      " [4.43 3.91]\n",
      " [1.24 3.12]\n",
      " [3.89 4.91]\n",
      " [3.99 1.65]\n",
      " [1.84 2.32]\n",
      " [2.7  2.27]\n",
      " [2.2  3.34]\n",
      " [4.6  2.24]\n",
      " [3.02 1.94]]\n",
      "[[3.51 2.  ]\n",
      " [3.   2.  ]\n",
      " [1.   2.  ]]\n",
      "[3.51 4.47]\n",
      "μx*:\n",
      "[0.76909309 0.75502312 0.80738742]\n",
      "Σx**:\n",
      "[[0.18824243 0.05007778 0.00460365]\n",
      " [0.05007778 0.06071811 0.00229454]\n",
      " [0.00460365 0.00229454 0.62650824]]\n",
      "μn* (existing):\n",
      "[0.76163948]\n",
      "Σn* (existing):\n",
      "[[0.16823853]]\n",
      "μn* (new):\n",
      "[0.81442183]\n",
      "Σn* (new):\n",
      "[[0.36071811]]\n"
     ]
    }
   ],
   "source": [
    "new_x = np.array([[observed_configs_list[0,0],2],[3,2],[1,2]]) # Placeholer\n",
    "print(observed_configs_list)\n",
    "print(new_x)\n",
    "k_x_n,k_x_star,k_x_star_star = compute_k_x(observed_configs_list,new_x,kernel_global)\n",
    "k_x_inv = np.linalg.inv(k_x_n)\n",
    "# print(f\"Kx:\\n{k_x}\\n Kx*:\\n{k_x_star}\\n Kx**:\\n{k_x_star_star}\")\n",
    "\n",
    "new_x_n=np.array([100]) # Placeholer\n",
    "curve_n=observed_configs_dicts[\"_\".join([str(x) for x in observed_configs_list[0]])]\n",
    "print(observed_configs_list[0])\n",
    "# print(curve)\n",
    "# print(new_x_n)\n",
    "k_t_n,k_t_n_star,k_t_n_star_star=compute_k_t_n(curve_n[0],new_x_n,kernel_local)\n",
    "k_t_n_inv=np.linalg.inv(k_t_n)\n",
    "# print(f\"Ktn:\\n{k_t_n}\\n Ktn*:\\n{k_t_n_star}\\n Ktn**:\\n{k_t_n_star_star}\")\n",
    "\n",
    "k_t=compute_k_t(observed_configs_list,observed_configs_dicts) # Placeholer\n",
    "k_t_inv=np.linalg.inv(k_t)\n",
    "# print(f\"Kt:\\n {k_t}\")\n",
    "\n",
    "o = compute_o(observed_configs_list,observed_configs_dicts)\n",
    "# print(f\"O:\\n{o}\")\n",
    "\n",
    "lambd = compute_lambda(o,k_t_inv)\n",
    "lambd_inv = np.linalg.inv(lambd)\n",
    "# print(f\"Lambda:\\n{lambd}\")\n",
    "\n",
    "means_vec = constant_mean(observed_configs_list)\n",
    "# print(f\"Means:\\n{means_vec}\")\n",
    "\n",
    "y_vec = compute_y_vector(observed_configs_list,observed_configs_dicts)\n",
    "# print(f\"Y:\\n{y_vec}\")\n",
    "\n",
    "gamma = compute_gamma(o,k_t_inv,y_vec,means_vec)\n",
    "# print(f\"Gamma:\\n{gamma}\")\n",
    "\n",
    "c = compute_c(k_x_inv,lambd)\n",
    "# print(f\"C:\\n{c}\")\n",
    "\n",
    "mu_global = compute_mu(means_vec,c,gamma)\n",
    "# print(f\"Mu:\\n{mu_global}\")\n",
    "\n",
    "omega_n = compute_omega(k_t_n_star,k_t_n_inv)\n",
    "# print(f\"Omega n:\\n{omega_n}\")\n",
    "\n",
    "mu = compute_mu_x_star(constant_mean(new_x),k_x_star,k_x_inv,mu_global,means_vec)\n",
    "print(f\"μx*:\\n{mu}\")\n",
    "\n",
    "var = compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x_n,lambd_inv)\n",
    "print(f\"Σx**:\\n{var}\")\n",
    "\n",
    "mu_n_star_ex = compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,curve_n[1],omega_n,mu_global[0])\n",
    "print(f\"μn* (existing):\\n{mu_n_star_ex}\")\n",
    "\n",
    "sigma_n_star_ex = compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,c[0,0])\n",
    "print(f\"Σn* (existing):\\n{sigma_n_star_ex}\")\n",
    "\n",
    "mu_n_star_new = compute_mu_n_star_new(mu_global[1],np.array([1]))\n",
    "print(f\"μn* (new):\\n{mu_n_star_new}\")\n",
    "\n",
    "sigma_n_star_new = compute_sigma_n_star_new(compute_k_x_star_star(np.array([1]),kernel_local),var[1,1])\n",
    "print(f\"Σn* (new):\\n{sigma_n_star_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_EI_at_x(mu,var,best_mu):\n",
    "    z=(best_mu-mu)/np.sqrt(var)\n",
    "    return np.sqrt(var)*(z*sc.stats.norm.cdf(z))+sc.stats.norm.pdf(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Our Bayesian optimization strategy proceeds by maintaining a basket of B = Bold + Bnew candidate\n",
    "models. Bold represents some number of models that have already been trained to some degree, while Bnew\n",
    "represents some number of brand new models. In practice, we set Bold = 10 and Bnew = 3. The entire\n",
    "basket is chosen using models with the maximum EI at the asymptote, which is computed using Equations\n",
    "19 and 3. Each round, after a new observation has been collected, the basket is re-built using possibly\n",
    "different models. This step is essentially standard Bayesian optimization using EI.\n",
    "'''\n",
    "\n",
    "# Fill the basket with configs\n",
    "basket_new=np.empty((0,len(bounds.keys())))\n",
    "basket_old=np.empty((0,len(bounds.keys())))\n",
    "basket_new_mu_var=[]\n",
    "basket_old_mu_var=[]\n",
    "basket_old_c=[]\n",
    "\n",
    "# Get the best yet observed configuration\n",
    "best_observation = np.min(np.concatenate([observed_configs_dicts['_'.join([str(c) for c in config])][1] for config in observed_configs_list]))\n",
    "# print(f\"Best observation: {best_observation}\")\n",
    "\n",
    "# Calculate EI for many configs to find the best ones\n",
    "\n",
    "# Sample N_EI_SAMPLES new configurations\n",
    "ei_configs = []\n",
    "for i in range(EI_N_SAMPLES):\n",
    "    while True:\n",
    "        new_config=np.empty(0)\n",
    "        for key in bounds.keys():\n",
    "            new_config=np.append(new_config,np.round(np.random.uniform(bounds[key][0],bounds[key][1]),2))\n",
    "        if not new_config in observed_configs_list:\n",
    "            break\n",
    "    ei_configs.append(new_config)\n",
    "if len(basket_old)<B_OLD:\n",
    "    ei_configs = np.concatenate([ei_configs,observed_configs_list])\n",
    "ei_configs=np.array(ei_configs)\n",
    "# print(ei_configs)\n",
    "\n",
    "# Calculate the mean and variance at the asymptote for each config using equation 19\n",
    "k_x_n,k_x_star,k_x_star_star = compute_k_x(observed_configs_list,ei_configs,kernel_global)\n",
    "k_x_inv = np.linalg.inv(k_x_n)\n",
    "k_t=compute_k_t(observed_configs_list,observed_configs_dicts)\n",
    "k_t_inv=np.linalg.inv(k_t)\n",
    "means_vec = constant_mean(observed_configs_list)\n",
    "o=compute_o(observed_configs_list,observed_configs_dicts)\n",
    "lambd = compute_lambda(o,k_t_inv)\n",
    "lambd_inv = np.linalg.inv(lambd)\n",
    "c=compute_c(k_x_inv,lambd)\n",
    "y_vec = compute_y_vector(observed_configs_list,observed_configs_dicts)\n",
    "gamma = compute_gamma(o,k_t_inv,y_vec,means_vec)\n",
    "mu_global = compute_mu(means_vec,c,gamma)\n",
    "mu = compute_mu_x_star(constant_mean(ei_configs),k_x_star,k_x_inv,mu_global,means_vec)\n",
    "# print(f\"μx*:\\n{mu}\")\n",
    "cov = compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x_n,lambd_inv)\n",
    "var=np.diag(cov)\n",
    "# print(f\"Σx**:\\n{var}\")\n",
    "\n",
    "# Calculate the EI scores for each config using equation 3\n",
    "ei_scores=compute_EI_at_x(mu,var,best_observation)\n",
    "sort_indices = np.argsort(ei_scores)[::-1]\n",
    "ei_configs_ranked = ei_configs[sort_indices]\n",
    "# print(ei_configs_ranked)\n",
    "\n",
    "\n",
    "# Greedily choose the best HP-config using Equation 19 & 3 until B_OLD existing configs and B_NEW new configs are found\n",
    "\n",
    "for n_sample,sampled_EI_config in enumerate(ei_configs_ranked):\n",
    "    # Sample another config from EI and try to add it to the basket\n",
    "    # If it is already in the basket, or the basket is full, skip it\n",
    "    if not np.any(np.all(np.isin(observed_configs_list,sampled_EI_config),axis=1)) and (basket_new.shape[0]==0 or B_NEW>basket_new.shape[0] and not np.any(np.all(np.isin(basket_new,sampled_EI_config),axis=1))):\n",
    "        # print(f\"Adding new config to basket_new: {sampled_EI_config}\")\n",
    "        basket_new=np.vstack([basket_new,sampled_EI_config])\n",
    "        basket_new_mu_var.append([mu[n_sample],var[n_sample]])\n",
    "    elif np.any(np.all(np.isin(observed_configs_list,sampled_EI_config),axis=1)) and (basket_old.shape[0]==0 or B_OLD>basket_old.shape[0] and not np.any(np.all(np.isin(basket_old,sampled_EI_config),axis=1))):\n",
    "        # print(f\"Adding new config to basket_old: {sampled_EI_config}\")\n",
    "        basket_old=np.vstack([basket_old,sampled_EI_config])\n",
    "        basket_old_c.append(c[np.where((observed_configs_list==sampled_EI_config).all(axis=1)),np.where((observed_configs_list==sampled_EI_config).all(axis=1))])\n",
    "        basket_old_mu_var.append([mu[n_sample],var[n_sample]])\n",
    "\n",
    "basket_new_mu_var=np.array(basket_new_mu_var)\n",
    "basket_old_mu_var=np.array(basket_old_mu_var)\n",
    "basket_old_c=np.array(basket_old_c)\n",
    "baskets_combined=np.vstack([basket_new,basket_old])\n",
    "\n",
    "\n",
    "# print(f\"New basket:\\n{basket_new}\")\n",
    "# print(f\"Old basket:\\n{basket_old}\")\n",
    "# print(f\"New basket mu var:\\n{basket_new_mu_var}\")\n",
    "# print(f\"Old basket mu var:\\n{basket_old_mu_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of P_min: 2.298286582802216\n"
     ]
    }
   ],
   "source": [
    "# Compute the entropy of the basket via Monte Carlo sampling\n",
    "baskets_mu_var=np.concatenate([basket_new_mu_var,basket_old_mu_var])\n",
    "h_p_min=compute_entropy(baskets_mu_var[:,0],baskets_mu_var[:,1],N_SAMPLES_MC)\n",
    "print(f\"Entropy of P_min: {h_p_min}\")\n",
    "a=np.zeros(basket_new.shape[0]+basket_old.shape[0])\n",
    "pred_epochs=np.linspace(1,PRED_EPOCH,PRED_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen new config: [2.79 2.55]\n",
      "Chosen new config: [2.88 2.77]\n",
      "Chosen new config: [2.71 2.6 ]\n"
     ]
    }
   ],
   "source": [
    "# For each config in the basket, N_FANT times fantasize an observation and recompute the information gain from it, collecting it in a\n",
    "for k_config,chosen_config in enumerate(basket_new):\n",
    "    \n",
    "    # Fantasize an observation using Equation 21\n",
    "    print(f\"Chosen new config: {chosen_config}\")\n",
    "    mu_n_star_new = compute_mu_n_star_new(basket_new_mu_var[k_config][0],pred_epochs)\n",
    "    # print(f\"μn* (new):\\n{mu_n_star_new}\")\n",
    "    sigma_n_star_new = compute_sigma_n_star_new(compute_k_x_star_star(pred_epochs,kernel_local),basket_new_mu_var[k_config][1])\n",
    "    # print(f\"Σn* (new):\\n{sigma_n_star_new}\")\n",
    "\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using the mu and sigma of the new config\n",
    "        fantasized_observation = np.random.multivariate_normal(mu_n_star_new,sigma_n_star_new)\n",
    "        # print(f\"Fantasized observation: {fantasized_observation}\")\n",
    "\n",
    "        # Compute the global mus and sigmas now including the fantasized observation\n",
    "        observations_incl_list=np.vstack([observed_configs_list,chosen_config])\n",
    "        observations_incl_dicts=observed_configs_dicts.copy()\n",
    "        observations_incl_dicts['_'.join([str(c) for c in chosen_config])]=(pred_epochs,fantasized_observation)\n",
    "        # print(observations_incl_dicts['_'.join([str(c) for c in chosen_config])])\n",
    "\n",
    "        k_x_n_incl,k_x_star_incl,k_x_star_star_incl = compute_k_x(observations_incl_list,baskets_combined,kernel_global)\n",
    "        k_x_inv_incl = np.linalg.inv(k_x_n_incl)\n",
    "        k_t_incl=compute_k_t(observations_incl_list,observations_incl_dicts)\n",
    "        k_t_inv_incl=np.linalg.inv(k_t_incl)\n",
    "        means_vec_incl = constant_mean(observations_incl_list)\n",
    "        o=compute_o(observations_incl_list,observations_incl_dicts)\n",
    "        lambd_incl = compute_lambda(o,k_t_inv_incl)\n",
    "        lambd_inv_incl = np.linalg.inv(lambd_incl)\n",
    "        c_incl=compute_c(k_x_inv_incl,lambd_incl)\n",
    "        y_vec_incl = compute_y_vector(observations_incl_list,observations_incl_dicts)\n",
    "        gamma_incl = compute_gamma(o,k_t_inv_incl,y_vec_incl,means_vec_incl)\n",
    "        mu_global_incl = compute_mu(means_vec_incl,c_incl,gamma_incl)\n",
    "        mu_y = compute_mu_x_star(constant_mean(baskets_combined),k_x_star_incl,k_x_inv_incl,mu_global_incl,means_vec_incl)\n",
    "        # print(f\"μx*:\\n{mu}\")\n",
    "        cov_incl = compute_sigma_x_star_star(k_x_star_star_incl,k_x_star_incl,k_x_n_incl,lambd_inv_incl)\n",
    "        var_y=np.diag(cov_incl)\n",
    "        # print(f\"Σx**:\\n{var}\")\n",
    "        \n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        h_p_min_y=compute_entropy(mu_y,var_y,N_SAMPLES_MC)\n",
    "        # print(f\"Entropy of P_min: {h_p_min_y}\")\n",
    "        a[k_config]+=(h_p_min_y-h_p_min)/N_FANT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen existing config: [2.7  2.27]\n",
      "Chosen existing config: [3.02 1.94]\n",
      "Chosen existing config: [3.51 4.47]\n",
      "Chosen existing config: [2.2  3.34]\n",
      "Chosen existing config: [3.89 4.91]\n",
      "Chosen existing config: [3.99 1.65]\n",
      "Chosen existing config: [1.84 2.32]\n",
      "Chosen existing config: [1.24 3.12]\n",
      "Chosen existing config: [4.43 3.91]\n",
      "Chosen existing config: [4.6  2.24]\n",
      "Best config: [2.7  2.27] (old)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k_config,chosen_config in enumerate(basket_old):\n",
    "    print(f\"Chosen existing config: {chosen_config}\")\n",
    "    pred_epochs_n=pred_epochs+observed_configs_dicts['_'.join([str(c) for c in chosen_config])][0][-1]\n",
    "    curve_n=observed_configs_dicts['_'.join([str(c) for c in chosen_config])]\n",
    "\n",
    "    k_t_n,k_t_n_star,k_t_n_star_star=compute_k_t_n(curve_n[0],pred_epochs_n,kernel_local)\n",
    "    k_t_n_inv=np.linalg.inv(k_t_n)\n",
    "    omega_n = compute_omega(k_t_n_star,k_t_n_inv)\n",
    "    mu_n_star_ex = compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,curve_n[1],omega_n,basket_old_mu_var[k_config][0])\n",
    "    # print(f\"μn* (existing):\\n{mu_n_star_ex}\")\n",
    "    sigma_n_star_ex = compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,basket_old_c[k_config])\n",
    "    # print(f\"Σn* (existing):\\n{sigma_n_star_ex}\")\n",
    "\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 20\n",
    "        # TODO fantasize an observation\n",
    "        fantasized_observation = np.random.multivariate_normal(mu_n_star_new,sigma_n_star_new)\n",
    "        # print(f\"Fantasized observation: {fantasized_observation}\")\n",
    "\n",
    "        # Compute the global mus and sigmas now including the fantasized observation\n",
    "        observations_incl_list=observed_configs_list\n",
    "        observations_incl_dicts=observed_configs_dicts.copy()\n",
    "        old_config_entry=observations_incl_dicts['_'.join([str(c) for c in chosen_config])]\n",
    "        observations_incl_dicts['_'.join([str(c) for c in chosen_config])]=(np.append(old_config_entry[0],pred_epochs_n),np.append(old_config_entry[1],fantasized_observation))\n",
    "        # print(observations_incl_dicts['_'.join([str(c) for c in chosen_config])])\n",
    "\n",
    "        k_x_n_incl,k_x_star_incl,k_x_star_star_incl = compute_k_x(observations_incl_list,baskets_combined,kernel_global)\n",
    "        k_x_inv_incl = np.linalg.inv(k_x_n_incl)\n",
    "        k_t_incl=compute_k_t(observations_incl_list,observations_incl_dicts)\n",
    "        k_t_inv_incl=np.linalg.inv(k_t_incl)\n",
    "        means_vec_incl = constant_mean(observations_incl_list)\n",
    "        o=compute_o(observations_incl_list,observations_incl_dicts)\n",
    "        lambd_incl = compute_lambda(o,k_t_inv_incl)\n",
    "        lambd_inv_incl = np.linalg.inv(lambd_incl)\n",
    "        c_incl=compute_c(k_x_inv_incl,lambd_incl)\n",
    "        y_vec_incl = compute_y_vector(observations_incl_list,observations_incl_dicts)\n",
    "        gamma_incl = compute_gamma(o,k_t_inv_incl,y_vec_incl,means_vec_incl)\n",
    "        mu_global_incl = compute_mu(means_vec_incl,c_incl,gamma_incl)\n",
    "        mu_y = compute_mu_x_star(constant_mean(baskets_combined),k_x_star_incl,k_x_inv_incl,mu_global_incl,means_vec_incl)\n",
    "        # print(f\"μx*:\\n{mu}\")\n",
    "        cov_incl = compute_sigma_x_star_star(k_x_star_star_incl,k_x_star_incl,k_x_n_incl,lambd_inv_incl)\n",
    "        var_y=np.diag(cov_incl)\n",
    "        # print(f\"Σx**:\\n{var}\")\n",
    "        \n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        h_p_min_y=compute_entropy(mu_y,var_y,N_SAMPLES_MC)\n",
    "        # print(f\"Entropy of P_min: {h_p_min_y}\")\n",
    "\n",
    "        a[k_config+B_NEW]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "\n",
    "# Select the config with the highest information gain\n",
    "best_config=(np.concatenate([basket_new,basket_old]))[np.argmax(a)]\n",
    "print(f\"Best config: {best_config} ({'new' if np.argmax(a)<B_NEW else 'old'})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_11_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
