{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing freeze-thaw bayesian optimization with two-level Gaussian processes\n",
    "# A global GP models the asymptotic mean of the learning curves of each HP-config\n",
    "# Local GPs model the learning curves of each HP-config\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, Kernel, Sum, WhiteKernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ft_testfunction import global_xD, local_xD\n",
    "import scipy as sc\n",
    "\n",
    "# Hyper-Hyperparameters of Freeze-Thaw Bayesian Optimization\n",
    "ALPHA,BETA = 1,0.5\n",
    "NOISE = 0.1\n",
    "B_OLD=2 # 10\n",
    "B_NEW=2 # 3\n",
    "MAX_EPOCHS=100\n",
    "EPOCH_STEP=1\n",
    "N_SAMPLES_MC=10 # number of samples for Monte Carlo integration\n",
    "N_FANT=5 # number of observations we fantasize for each point in the basket\n",
    "N_INIT_CONFIGS=10 # number of random initializations for the optimization\n",
    "N_INIT_EPOCHS=5 # number of epochs trained for initial configs\n",
    "INFERRED_MEAN = 0.8 # inferred mean of the global GP\n",
    "MATER_NU = 2.5 # Matern kernel parameter\n",
    "\n",
    "# Meta-Parameters of the task\n",
    "OBS_MIN = 0 # minimal loss value\n",
    "OBS_MAX = 1 # maximal loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquation 3: Expected Improvement-formula\\nEquation 4: Entropy Search-formula\\nEquation 5&6: Exponential Decay-kernel for Gaussian Process\\nEquation 12,13,17 & 18: Posterior distribution of global Gaussian Process\\nEquation 14 & 19: Posterior predictive distribution of global Gaussian Process\\nEquation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\\nEquation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equation 3: Expected Improvement-formula\n",
    "Equation 4: Entropy Search-formula\n",
    "Equation 5&6: Exponential Decay-kernel for Gaussian Process\n",
    "Equation 12,13,17 & 18: Posterior distribution of global Gaussian Process\n",
    "Equation 14 & 19: Posterior predictive distribution of global Gaussian Process\n",
    "Equation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\n",
    "Equation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of exponential-decay kernel for local GPs\n",
    "class ExponentialDecayNoiseKernel(Kernel):\n",
    "    def __init__(self, alpha=1.0,beta=0.5,noise=0.1):\n",
    "        self.beta=beta\n",
    "        self.alpha=alpha\n",
    "        self.noise = noise\n",
    "    def __call__(self, X, Y=None,eval_gradient=False):\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        # if eval_gradient:\n",
    "        #     return (self.beta**self.alpha)/((X.flatten()[:,None]+Y.flatten()[None,:])+self.beta)**self.alpha,np.identity(X.shape[0])\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        return ((self.beta**self.alpha)/((X[None,:]+Y[:,None])+self.beta)**self.alpha + self.noise*np.where(X[None,:]-Y[:,None] == 0, 1, 0)).T\n",
    "    def diag(self, X):\n",
    "        return np.diag(self(X))\n",
    "    def is_stationary(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3.13': (array([1., 2., 3., 4., 5.]), array([0.93, 0.85, 0.82, 0.75, 0.69])), '1.63': (array([1., 2., 3., 4., 5.]), array([0.94, 0.89, 0.82, 0.78, 0.73])), '4.05': (array([1., 2., 3., 4., 5.]), array([0.95, 0.89, 0.83, 0.75, 0.74])), '1.97': (array([1., 2., 3., 4., 5.]), array([0.95, 0.88, 0.83, 0.79, 0.74])), '3.76': (array([1., 2., 3., 4., 5.]), array([0.92, 0.88, 0.81, 0.78, 0.7 ])), '2.73': (array([1., 2., 3., 4., 5.]), array([0.94, 0.86, 0.8 , 0.76, 0.69])), '3.79': (array([1., 2., 3., 4., 5.]), array([0.94, 0.86, 0.79, 0.76, 0.7 ])), '1.8': (array([1., 2., 3., 4., 5.]), array([0.94, 0.89, 0.82, 0.76, 0.75])), '3.71': (array([1., 2., 3., 4., 5.]), array([0.92, 0.87, 0.8 , 0.74, 0.71])), '3.64': (array([1., 2., 3., 4., 5.]), array([0.93, 0.88, 0.84, 0.78, 0.71]))}\n",
      "[[3.13]\n",
      " [1.63]\n",
      " [4.05]\n",
      " [1.97]\n",
      " [3.76]\n",
      " [2.73]\n",
      " [3.79]\n",
      " [1.8 ]\n",
      " [3.71]\n",
      " [3.64]]\n"
     ]
    }
   ],
   "source": [
    "# Start by observing N_INIT_CONFIGS random configurations for N_INIT_EPOCHS epoch each\n",
    "bounds={'HP1':(1.,5.)}#,'HP2':(1.,5.)}\n",
    "observed_configs_dicts={}\n",
    "observed_configs_list=[]\n",
    "for i in range(N_INIT_CONFIGS):\n",
    "    new_config=np.empty(0)\n",
    "    for key in bounds.keys():\n",
    "        new_config=np.append(new_config,np.round(np.random.uniform(bounds[key][0],bounds[key][1]),2))\n",
    "    \n",
    "    # Observe the new configuration for N_INIT_EPOCHS epochs\n",
    "    f_space = np.linspace(1,N_INIT_EPOCHS,N_INIT_EPOCHS)\n",
    "    experimental_data=local_xD(np.array(new_config),f_space,noise=0.01)\n",
    "    observed_configs_dicts['_'.join([str(config) for config in new_config])]=(f_space,experimental_data)\n",
    "\n",
    "\n",
    "    observed_configs_list.append(new_config)\n",
    "observed_configs_list=np.array(observed_configs_list)\n",
    "\n",
    "\n",
    "\n",
    "print(observed_configs_dicts)\n",
    "print(observed_configs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from botorch.models import SingleTaskGP\n",
    "# from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "# from gpytorch.means import ConstantMean\n",
    "# import torch\n",
    "#MaternKernel(nu=2.5)\n",
    "# print(torch.from_numpy(observed_configs_list))\n",
    "\n",
    "\n",
    "# Define the kernel for the global GP\n",
    "kernel_global = Matern(nu=MATER_NU)\n",
    "kernel_local = ExponentialDecayNoiseKernel(alpha=ALPHA,beta=BETA,noise=NOISE)\n",
    "\n",
    "def compute_k_x(x,new_x,kernel):\n",
    "    k_x = kernel_global(x)\n",
    "    k_x_star = kernel_global(x,new_x)\n",
    "    k_star_star = kernel_global(new_x)\n",
    "    return k_x,k_x_star,k_star_star\n",
    "\n",
    "def compute_k_t_n(x,new_x,kernel):\n",
    "    k_t=kernel(x)\n",
    "    k_t_star=kernel(x,new_x)\n",
    "    k_star_star=kernel(new_x)\n",
    "    return k_t,k_t_star,k_star_star\n",
    "\n",
    "def compute_k_t(k_ts):\n",
    "    return sc.linalg.block_diag(*k_ts)\n",
    "\n",
    "def compute_o(config_list,config_dict):\n",
    "    o=[]\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        o=sc.linalg.block_diag(*([o,np.ones((len(observations),1))]))\n",
    "    return o[1:]\n",
    "\n",
    "def compute_lambda(o,k_t_inv):\n",
    "    return o.T@k_t_inv@o\n",
    "\n",
    "def compute_gamma(o,k_t_inv,y,m):\n",
    "    return o.T@k_t_inv@(y-o@m)\n",
    "\n",
    "def constant_mean(x):\n",
    "    return INFERRED_MEAN*np.ones(x.shape[0])\n",
    "\n",
    "def compute_y_vector(config_list,config_dict):\n",
    "    y_vec=np.empty(0)\n",
    "    for config in config_list:\n",
    "        observations=config_dict['_'.join([str(c) for c in config])][1]\n",
    "        y_vec=np.append(y_vec,observations)\n",
    "    return y_vec\n",
    "\n",
    "def compute_c(k_x_inv,lambd):\n",
    "    return np.linalg.inv(k_x_inv+lambd)\n",
    "\n",
    "def compute_omega(k_t_n_star,k_t_n_inv):\n",
    "    obs_steps=k_t_n_inv.shape[0]\n",
    "    predict_steps=k_t_n_star.shape[1]\n",
    "    return np.ones(predict_steps)-k_t_n_star.T@k_t_n_inv@np.ones(obs_steps)\n",
    "\n",
    "def compute_mu(m,c,gamma):\n",
    "    return (m+c@gamma)\n",
    "\n",
    "# Equation 14/19:\n",
    "def compute_mu_x_star(m,k_x_star,k_x_inv,my,means_vec):\n",
    "    return m+k_x_star.T@k_x_inv@(my-means_vec)\n",
    "def compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x,lambd_inv):\n",
    "    return k_x_star_star-k_x_star.T@np.linalg.inv(k_x+lambd_inv)@k_x_star\n",
    "\n",
    "# Equation 16/21:\n",
    "def compute_sigma_n_star_new(k_t_star_star,sigma_star_star):\n",
    "    return k_t_star_star-sigma_star_star\n",
    "\n",
    "# Equation 15/20:\n",
    "def compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,y_n,omega_n,mu_n):\n",
    "    return k_t_n_star.T@k_t_n_inv@y_n+(omega_n*mu_n)\n",
    "def compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,c_nn):\n",
    "    return k_t_n_star_star-k_t_n_star.T@k_t_n_inv@k_t_n_star+omega_n@(c_nn*omega_n.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.13]\n",
      " [1.63]\n",
      " [4.05]\n",
      " [1.97]\n",
      " [3.76]\n",
      " [2.73]\n",
      " [3.79]\n",
      " [1.8 ]\n",
      " [3.71]\n",
      " [3.64]]\n",
      "μx*:\n",
      "[0.75582842 0.75156438 0.78687918]\n",
      "Σx**:\n",
      "[[ 6.15892616e-02  3.41423603e-02 -1.03909138e-02]\n",
      " [ 3.41423603e-02  4.89731504e-02  4.32966063e-04]\n",
      " [-1.03909138e-02  4.32966063e-04  4.55371490e-01]]\n",
      "μn* (existing):\n",
      "[0.77268138 0.76190711 0.75253797 0.7521358 ]\n",
      "Σn* (existing):\n",
      "[[0.22598291 0.12448997 0.11752693 0.11575169]\n",
      " [0.12448997 0.22423355 0.11833211 0.11629813]\n",
      " [0.11752693 0.11833211 0.21765606 0.11645047]\n",
      " [0.11575169 0.11629813 0.11645047 0.21585482]]\n"
     ]
    }
   ],
   "source": [
    "new_x = np.array([[2.69],[3],[1]]) # Placeholer\n",
    "print(observed_configs_list)\n",
    "# print(new_x)\n",
    "k_x,k_x_star,k_x_star_star = compute_k_x(observed_configs_list,new_x,kernel_global)\n",
    "k_x_inv = np.linalg.inv(k_x)\n",
    "# print(f\"Kx:\\n{k_x}\\n Kx*:\\n{k_x_star}\\n Kx**:\\n{k_x_star_star}\")\n",
    "\n",
    "new_x_n=np.array([6,10,50,100]) # Placeholer\n",
    "curve=observed_configs_dicts[\"_\".join([str(x) for x in observed_configs_list[0]])]\n",
    "# print(curve)\n",
    "# print(new_x_n)\n",
    "k_t_n,k_t_n_star,k_t_n_star_star=compute_k_t_n(curve[0],new_x_n,kernel_local)\n",
    "k_t_n_inv=np.linalg.inv(k_t_n)\n",
    "# print(f\"Ktn:\\n{k_t_n}\\n Ktn*:\\n{k_t_n_star}\\n Ktn**:\\n{k_t_n_star_star}\")\n",
    "\n",
    "k_t=compute_k_t([k_t_n]*len(observed_configs_list)) # Placeholer\n",
    "k_t_inv=np.linalg.inv(k_t)\n",
    "# print(f\"Kt:\\n {k_t}\")\n",
    "\n",
    "o = compute_o(observed_configs_list,observed_configs_dicts)\n",
    "# print(f\"O:\\n{o}\")\n",
    "\n",
    "lambd = compute_lambda(o,k_t_inv)\n",
    "lambd_inv = np.linalg.inv(lambd)\n",
    "# print(f\"Lambda:\\n{lambd}\")\n",
    "\n",
    "means_vec = constant_mean(observed_configs_list)\n",
    "# print(f\"Means:\\n{means_vec}\")\n",
    "\n",
    "y_vec = compute_y_vector(observed_configs_list,observed_configs_dicts)\n",
    "# print(f\"Y:\\n{y_vec}\")\n",
    "\n",
    "gamma = compute_gamma(o,k_t_inv,y_vec,means_vec)\n",
    "# print(f\"Gamma:\\n{gamma}\")\n",
    "\n",
    "c = compute_c(k_x_inv,lambd)\n",
    "# print(f\"C:\\n{c}\")\n",
    "\n",
    "mu_global = compute_mu(means_vec,c,gamma)\n",
    "# print(f\"Mu:\\n{mu_global}\")\n",
    "\n",
    "omega_n = compute_omega(k_t_n_star,k_t_n_inv)\n",
    "# print(f\"Omega n:\\n{omega_n}\")\n",
    "\n",
    "mu_x_star = compute_mu_x_star(constant_mean(new_x),k_x_star,k_x_inv,mu_global,means_vec)\n",
    "print(f\"μx*:\\n{mu_x_star}\")\n",
    "\n",
    "sigma_x_star_star = compute_sigma_x_star_star(k_x_star_star,k_x_star,k_x,lambd_inv)\n",
    "print(f\"Σx**:\\n{sigma_x_star_star}\")\n",
    "\n",
    "mu_n_star_ex = compute_mu_n_star_ex(k_t_n_star,k_t_n_inv,curve[1],omega_n,mu_global[0])\n",
    "print(f\"μn* (existing):\\n{mu_n_star_ex}\")\n",
    "\n",
    "sigma_n_star_ex = compute_sigma_n_star_ex(k_t_n_star_star,k_t_n_star,k_t_n_inv,omega_n,c[0,0])\n",
    "print(f\"Σn* (existing):\\n{sigma_n_star_ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.2])] [array([2.73]), array([3.76])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Our Bayesian optimization strategy proceeds by maintaining a basket of B = Bold + Bnew candidate\n",
    "models. Bold represents some number of models that have already been trained to some degree, while Bnew\n",
    "represents some number of brand new models. In practice, we set Bold = 10 and Bnew = 3. The entire\n",
    "basket is chosen using models with the maximum EI at the asymptote, which is computed using Equations\n",
    "19 and 3. Each round, after a new observation has been collected, the basket is re-built using possibly\n",
    "different models. This step is essentially standard Bayesian optimization using EI.\n",
    "'''\n",
    "# Fill the basket with configs\n",
    "basket_new=[]\n",
    "basket_old=[]\n",
    "\n",
    "\n",
    "# Greedily choose the best HP-config using Equation 19 & 3 until B_OLD existing configs and B_NEW new configs are found\n",
    "iterator=1.\n",
    "while not (len(basket_new)==B_NEW):\n",
    "    # Sample another config from EI and try to add it to the basket\n",
    "    # If it is already in the basket, or the basket is full, skip it\n",
    "    # TODO: Implement equations 19 & 3 here\n",
    "    # Get the iterator best configs from EI\n",
    "    sampled_EI_config = np.array([iterator]) # Take the iterator-th config from EI\n",
    "    if not sampled_EI_config in observed_configs_list and not sampled_EI_config in basket_new and len(basket_new)<B_NEW:\n",
    "        basket_new.append(sampled_EI_config)\n",
    "    elif sampled_EI_config in observed_configs_list and not sampled_EI_config in basket_old and len(basket_old)<B_OLD:\n",
    "        basket_old.append(sampled_EI_config)\n",
    "    iterator+=0.2\n",
    "if len(basket_old)<B_OLD:\n",
    "    # If we did not find enough existing configs, fill the rest of the basket with random existing configs\n",
    "    while len(basket_old)<B_OLD:\n",
    "        random_config = observed_configs_list[np.random.randint(0,len(observed_configs_list))]\n",
    "        if not random_config in basket_old:\n",
    "            basket_old.append(random_config)\n",
    "\n",
    "print(basket_new,basket_old)\n",
    "a=np.zeros(len(basket_new+basket_old))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute p_min over the basket using Monte Carlo simulation and Equation 19\n",
    "\n",
    "# p_min is the posterior predictive distribution over the minimum of the global GP\n",
    "# We will use it's entropy to observe configs with results that make the p_min distribution spikier\n",
    "\n",
    "# Monte Carlo simulation\n",
    "# Using the global GP, we create a discrete grid of the N_SAMPLES_MC highest EI configs\n",
    "# From the grid, compute P_min and the entropy of P_min, H(P_min)\n",
    "\n",
    "h_p_min=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# For each config in the basket, N_FANT times fantasize an observation and recompute the information gain from it, collecting it in a\n",
    "for k,config in enumerate(basket_new):\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 21\n",
    "        # TODO fantasize an observation\n",
    "\n",
    "        # Compute p_min_y, the new minimum distribution, including the fantasized observation y, again using Monte Carlo and Equation 19\n",
    "        # TODO compute p_min_y\n",
    "\n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        # TODO compute H(P_min_y)\n",
    "        h_p_min_y=0.5\n",
    "        # \n",
    "        a[k]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "for k,config in enumerate(basket_old):\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 20\n",
    "        # TODO fantasize an observation\n",
    "\n",
    "        # Compute p_min_y, the new minimum distribution, including the fantasized observation y, again using Monte Carlo and Equation 19\n",
    "        # TODO compute p_min_y\n",
    "\n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        # TODO compute H(P_min_y)\n",
    "        h_p_min_y=0\n",
    "\n",
    "        a[k+B_NEW]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "\n",
    "# Select the config with the highest information gain\n",
    "best_config=(basket_new+basket_old)[np.argmax(a)]\n",
    "print(best_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_11_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
