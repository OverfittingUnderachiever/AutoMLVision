{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing freeze-thaw bayesian optimization with two-level Gaussian processes\n",
    "# A global GP models the asymptotic mean of the learning curves of each HP-config\n",
    "# Local GPs model the learning curves of each HP-config\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, Kernel, Sum, WhiteKernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ALPHA,BETA = 1,0.5\n",
    "NOISE = 0.1\n",
    "B_OLD=2 # 10\n",
    "B_NEW=2 # 3\n",
    "MAX_EPOCHS=100\n",
    "EPOCH_STEP=1\n",
    "N_SAMPLES_MC=10 # number of samples for Monte Carlo integration\n",
    "N_FANT=5 # number of observations we fantasize for each point in the basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEquation 3: Expected Improvement-formula\\nEquation 4: Entropy Search-formula\\nEquation 5&6: Exponential Decay-kernel for Gaussian Process\\nEquation 12,13,17 & 18: Posterior distribution of global Gaussian Process\\nEquation 14 & 19: Posterior predictive distribution of global Gaussian Process\\nEquation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\\nEquation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Equation 3: Expected Improvement-formula\n",
    "Equation 4: Entropy Search-formula\n",
    "Equation 5&6: Exponential Decay-kernel for Gaussian Process\n",
    "Equation 12,13,17 & 18: Posterior distribution of global Gaussian Process\n",
    "Equation 14 & 19: Posterior predictive distribution of global Gaussian Process\n",
    "Equation 15 & 20: Posterior predictive distribution of a local Gaussian Process with existing oberservations of this HP-config\n",
    "Equation 16 & 21: Posterior predicitve distribution of a local Gaussian Process without existing oberservations of this HP-config\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.]], dtype=torch.float64)\n",
      "[[1.         0.30562244]\n",
      " [0.30562244 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "observed_configs_dicts=[{'HP1':0.},{'HP1':1.}]\n",
    "observed_configs_list=np.array([[0.],[1.]])\n",
    "\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define the kernel for the global GP\n",
    "kernel_global = MaternKernel(nu=2.5)\n",
    "print(torch.from_numpy(observed_configs_list))\n",
    "k_x = kernel_global(torch.from_numpy(observed_configs_list)).numpy()\n",
    "print(k_x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'HP1': 2}, {'HP1': 3}] [{'HP1': 0}, {'HP1': 1}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Our Bayesian optimization strategy proceeds by maintaining a basket of B = Bold + Bnew candidate\n",
    "models. Bold represents some number of models that have already been trained to some degree, while Bnew\n",
    "represents some number of brand new models. In practice, we set Bold = 10 and Bnew = 3. The entire\n",
    "basket is chosen using models with the maximum EI at the asymptote, which is computed using Equations\n",
    "19 and 3. Each round, after a new observation has been collected, the basket is re-built using possibly\n",
    "different models. This step is essentially standard Bayesian optimization using EI.\n",
    "'''\n",
    "# Fill the basket with configs\n",
    "basket_new=[]\n",
    "basket_old=[]\n",
    "\n",
    "\n",
    "# Greedily choose the best HP-config using Equation 19 & 3 until B_OLD existing configs and B_NEW new configs are found\n",
    "iterator=0\n",
    "while not (len(basket_new)==B_NEW and len(basket_old)==B_OLD):\n",
    "    # Sample another config from EI and try to add it to the basket\n",
    "    # If it is already in the basket, or the basket is full, skip it\n",
    "    # TODO: Implement equations 19 & 3 here\n",
    "    # Get the iterator best configs from EI\n",
    "    sampled_EI_config = {'HP1':iterator} # Take the iterator-th config from EI\n",
    "    if not sampled_EI_config in observed_configs_dicts and not sampled_EI_config in basket_new and len(basket_new)<B_NEW:\n",
    "        basket_new.append(sampled_EI_config)\n",
    "    elif sampled_EI_config in observed_configs_dicts and not sampled_EI_config in basket_old and len(basket_old)<B_OLD:\n",
    "        basket_old.append(sampled_EI_config)\n",
    "    iterator+=1\n",
    "\n",
    "\n",
    "print(basket_new,basket_old)\n",
    "a=np.zeros(len(basket_new+basket_old))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute p_min over the basket using Monte Carlo simulation and Equation 19\n",
    "\n",
    "# p_min is the posterior predictive distribution over the minimum of the global GP\n",
    "# We will use it's entropy to observe configs with results that make the p_min distribution spikier\n",
    "\n",
    "# Monte Carlo simulation\n",
    "# Using the global GP, we create a discrete grid of the N_SAMPLES_MC highest EI configs\n",
    "# From the grid, compute P_min and the entropy of P_min, H(P_min)\n",
    "\n",
    "h_p_min=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HP1': 2}\n"
     ]
    }
   ],
   "source": [
    "# For each config in the basket, N_FANT times fantasize an observation and recompute the information gain from it, collecting it in a\n",
    "for k,config in enumerate(basket_new):\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 21\n",
    "        # TODO fantasize an observation\n",
    "\n",
    "        # Compute p_min_y, the new minimum distribution, including the fantasized observation y, again using Monte Carlo and Equation 19\n",
    "        # TODO compute p_min_y\n",
    "\n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        # TODO compute H(P_min_y)\n",
    "        h_p_min_y=0.5\n",
    "        # \n",
    "        a[k]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "for k,config in enumerate(basket_old):\n",
    "    for f_n in range(N_FANT):\n",
    "        # Fantasize an observation using Equation 20\n",
    "        # TODO fantasize an observation\n",
    "\n",
    "        # Compute p_min_y, the new minimum distribution, including the fantasized observation y, again using Monte Carlo and Equation 19\n",
    "        # TODO compute p_min_y\n",
    "\n",
    "        # Compute the new entropy of p_min_y, H(P_min_y)\n",
    "        # TODO compute H(P_min_y)\n",
    "        h_p_min_y=0\n",
    "\n",
    "        a[k+B_NEW]+=(h_p_min_y-h_p_min)/N_FANT\n",
    "\n",
    "\n",
    "# Select the config with the highest information gain\n",
    "best_config=(basket_new+basket_old)[np.argmax(a)]\n",
    "print(best_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_11_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
